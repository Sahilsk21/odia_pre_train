{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1bb22RCRTXzMPLtKz5teeYbUyZkJj7cpr","authorship_tag":"ABX9TyPr8j9XReCgDRq5T4CRKcic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DdpUae5BLe8J"},"outputs":[],"source":["## the is the pre train llm model base on odia language\n","## this model follow llama2 and gpt architecture"]},{"cell_type":"code","source":["## tiktoken use for use python builtent tokenizer\n","\n","\n","!pip install torch\n","!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3crU4e2gL98M","executionInfo":{"status":"ok","timestamp":1727660989542,"user_tz":-330,"elapsed":6707,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"50327287-dc39-4ecb-9522-f3a70f6c0037"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y35e5mDWhiHG","executionInfo":{"status":"ok","timestamp":1727600356331,"user_tz":-330,"elapsed":4215,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"8ac0dc5a-0c87-48ba-de26-8595464489a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"]}]},{"cell_type":"code","source":["# load the odia language data set\n","with open(\"odia_data_ind.txt\", \"r\") as f:\n","    odia_language_data = f.read()\n","\n"],"metadata":{"id":"-17tkgg6McIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## in this model use gpt2 builten BPE TOKENIZER with 50,000 vocab size\n","## use 2 special token sos and eos\n","\n","import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"lC-KJXgUNi4y","executionInfo":{"status":"ok","timestamp":1727661010179,"user_tz":-330,"elapsed":6370,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["text=(\"ବାକ୍ୟ ବିଚାର</EOS>ଏମିତି ଗୋଟିଏ ଦିନ ଯାଉନି ଯେଉଁ ଦିନ ଆର୍ଟିଓ ଅଫିସରେ ଲମ୍ବା ଧାଡ଼ି ଦେଖିବାକୁ ମିଳୁନି ।\")"],"metadata":{"id":"gP_VtfDKOLj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["integers = tokenizer.encode(text, allowed_special={\"<SOS>\",\"</EOS>\"})\n","\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95AI-R08TB_L","executionInfo":{"status":"ok","timestamp":1727645587446,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"ce29b2c3-9c33-4ef5-8cc6-513748d40685"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[156, 105, 105, 156, 105, 122, 156, 105, 243, 156, 255, 235, 156, 255, 253, 220, 156, 105, 105, 156, 105, 123, 156, 105, 248, 156, 105, 122, 156, 105, 108, 3556, 36, 2640, 29, 156, 105, 237, 156, 105, 106, 156, 105, 123, 156, 105, 97, 156, 105, 123, 220, 156, 105, 245, 156, 255, 233, 156, 105, 253, 156, 105, 123, 156, 105, 237, 220, 156, 105, 99, 156, 105, 123, 156, 105, 101, 220, 156, 105, 107, 156, 105, 122, 156, 105, 231, 156, 105, 101, 156, 105, 123, 220, 156, 105, 107, 156, 255, 229, 156, 105, 231, 156, 105, 223, 220, 156, 105, 99, 156, 105, 123, 156, 105, 101, 220, 156, 105, 228, 156, 105, 108, 156, 255, 235, 156, 105, 253, 156, 105, 123, 156, 105, 241, 220, 156, 105, 227, 156, 105, 104, 156, 105, 123, 156, 105, 116, 156, 105, 108, 156, 255, 229, 220, 156, 105, 110, 156, 105, 106, 156, 255, 235, 156, 105, 105, 156, 105, 122, 220, 156, 105, 100, 156, 105, 122, 156, 105, 94, 156, 45539, 156, 105, 123, 220, 156, 105, 99, 156, 255, 229, 156, 105, 244, 156, 105, 123, 156, 105, 105, 156, 105, 122, 156, 105, 243, 156, 255, 223, 220, 156, 105, 106, 156, 105, 123, 156, 105, 111, 156, 255, 223, 156, 105, 101, 156, 105, 123, 220, 24231, 97]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(integers))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIwWqV-iTMlP","executionInfo":{"status":"ok","timestamp":1727645592800,"user_tz":-330,"elapsed":495,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"0aab9e55-c714-4497-c8bd-e6a3683d620a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ବାକ୍ୟ ବିଚାର</EOS>ଏମିତି ଗୋଟିଏ ଦିନ ଯାଉନି ଯେଉଁ ଦିନ ଆର୍ଟିଓ ଅଫିସରେ ଲମ୍ବା ଧାଡ଼ି ଦେଖିବାକୁ ମିଳୁନି ।\n"]}]},{"cell_type":"code","source":["## now impliment sliding window for self-supervise learning\n","## next word prediction\n","## use data loader\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"HyT6SIsGTR5n","executionInfo":{"status":"ok","timestamp":1727661026312,"user_tz":-330,"elapsed":3631,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class ODIA_Dataset(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"</EOS>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"mVD5QAKuUXbw","executionInfo":{"status":"ok","timestamp":1727661028684,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def create_dataloader(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = ODIA_Dataset(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"],"metadata":{"id":"6hLyoNLGU2Hi","executionInfo":{"status":"ok","timestamp":1727661035029,"user_tz":-330,"elapsed":713,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["## test our data loader\n","test_data=(\"ଆଇସିସି ବିଶ୍ବକପରେ ସେ ଅଂଶ ଗ୍ରହଣ କରିଥିବାରୁ ସେ ନିଜକୁ ଭାଗ୍ୟଶାଳୀ ମନେ କରୁଛନ୍ତି ।କରୀନା କପୂର ହେଉଛନ୍ତି ଭାରତୀୟ କ୍ରିକେଟ ଟିମର ପୂର୍ବତନ ଅଧିନାୟକ ମସୁର ଅଲ୍ଲୀ ଖାନଙ୍କ ପଟ୍ଟୌଦୀଙ୍କ ବୋହୂ ।କିନ୍ତୁ ଏସବୁ ସତ୍ତ୍ବେ ଆରଟିଓ ଅଫିସରେ ଏବେ ଲୋକଙ୍କ ଆଖିରେ ଆଙ୍ଗୁଠି ଗେଞ୍ଜି ଆବେଦନକାରୀ ପିଛା ୨୨ଟଙ୍କା ଲୁଟୁଛନ୍ତି ।ଗୁଗଲ ସୂଚନା ଅନୁଯାୟୀ ପରବର୍ତ୍ତୀ ମୂହୁର୍ତ୍ତରେ ଜୋସେଫ ପ୍ଲେଟ୍ୟୁ ନିଜ ଆଖିର ଜ୍ୟୋତି ହରାଇ ବସିଥିଲେ \")\n","\n","dataloader = create_dataloader(\n","    test_data, batch_size=1, max_length=10, stride=4, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma1WrvRcVFWg","executionInfo":{"status":"ok","timestamp":1727661044244,"user_tz":-330,"elapsed":722,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"a31e904a-a360-4825-8919-5ca4b2bafd61"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[156, 105, 228, 156, 105, 229, 156, 105, 116, 156]]), tensor([[105, 228, 156, 105, 229, 156, 105, 116, 156, 105]])]\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"fNyqp0NcVzh2","executionInfo":{"status":"ok","timestamp":1727589385683,"user_tz":-330,"elapsed":826,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"c73b52d7-14e2-450c-ef50-8b7f8860db1d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/odia_data.txt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["dataloader = create_dataloader(test_data, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"\\nTargets:\\n\", targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJNioGWGWNhd","executionInfo":{"status":"ok","timestamp":1727645615644,"user_tz":-330,"elapsed":519,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"5aa1ca87-784d-4db8-a2e0-d6ee1a386f2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputs:\n"," tensor([[156, 105, 228, 156],\n","        [105, 229, 156, 105],\n","        [116, 156, 105, 123],\n","        [156, 105, 116, 156],\n","        [105, 123, 220, 156],\n","        [105, 105, 156, 105],\n","        [123, 156, 105, 114],\n","        [156, 255, 235, 156]])\n","\n","Targets:\n"," tensor([[105, 228, 156, 105],\n","        [229, 156, 105, 116],\n","        [156, 105, 123, 156],\n","        [105, 116, 156, 105],\n","        [123, 220, 156, 105],\n","        [105, 156, 105, 123],\n","        [156, 105, 114, 156],\n","        [255, 235, 156, 105]])\n"]}]},{"cell_type":"code","source":["## now apply word emmbeding\n","## using pytorch builten emmbeding\n","\n","voc_size=50\n","emmbed_dim=10"],"metadata":{"id":"Aw_nn9p3XiqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","embedding_layer = torch.nn.Embedding(voc_size, emmbed_dim)"],"metadata":{"id":"jjH_J2MoX-A3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_layer(torch.tensor(1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXMwTEQvYESO","executionInfo":{"status":"ok","timestamp":1727589823667,"user_tz":-330,"elapsed":617,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"d1e60e16-d470-4f0d-9925-7c186cb1d2ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692, -0.0770, -1.0205,\n","        -0.1690,  0.9178], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["## 10 embeding\n","\n","## test with 50,000 vocab 256 emmbed\n","vocab_size = 50257\n","output_dim = 256\n","\n","token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"],"metadata":{"id":"juR3a_uUYK8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 40\n","dataloader = create_dataloader(\n","    odia_language_data, batch_size=10, max_length=max_length,\n","    stride=max_length, shuffle=False\n",")\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)"],"metadata":{"id":"2khv9MasYwVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_embeddings = token_embedding_layer(inputs)\n","print(token_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVwiLI5EY2a7","executionInfo":{"status":"ok","timestamp":1727592509284,"user_tz":-330,"elapsed":828,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"f3b89710-89c4-44a5-9de6-504689694020"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 40, 256])\n"]}]},{"cell_type":"code","source":["## position embeding\n","pos_embedding_layer = torch.nn.Embedding(10, 256)"],"metadata":{"id":"H5KTHT7eZKV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_embeddings = pos_embedding_layer(torch.arange(10))\n","print(pos_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vd42VO95ZZWa","executionInfo":{"status":"ok","timestamp":1727592866117,"user_tz":-330,"elapsed":571,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"8fe46e4d-b54a-40fa-9682-84f16bffbbe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 256])\n"]}]},{"cell_type":"code","source":["\n","## now apply multi head mask self attntion\n","import torch.nn as nn\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        # Reduce the projection dim to match desired output dim\n","        self.head_dim = d_out // num_heads\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        # Linear layer to combine head outputs\n","        self.out_proj = nn.Linear(d_out, d_out)\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","        # Shape: (b, num_tokens, d_out)\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"Hc5CkdpJjsuO","executionInfo":{"status":"ok","timestamp":1727661070743,"user_tz":-330,"elapsed":727,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["## create swiglu activation for hiden neural network layer\n","import torch.nn.functional as F\n","class SwiGLU(nn.Module):\n","    def forward(self, x):\n","        x, gate = x.chunk(2, dim=-1)\n","        return F.silu(gate) * x"],"metadata":{"id":"dZH9JBOckVsO","executionInfo":{"status":"ok","timestamp":1727661077390,"user_tz":-330,"elapsed":720,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["## create gelu activation for hiden neural network layer\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"],"metadata":{"id":"wbXF5aOWru-3","executionInfo":{"status":"ok","timestamp":1727661080627,"user_tz":-330,"elapsed":646,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["## apply RMSNorm for layer normalization\n","## it help reduce the computational\n","\n","class RMSNorm(nn.Module):\n","    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n","\n","        super(RMSNorm, self).__init__()\n","\n","        self.eps = eps\n","        self.d = d\n","        self.p = p\n","        self.bias = bias\n","\n","        self.scale = nn.Parameter(torch.ones(d))\n","        self.register_parameter(\"scale\", self.scale)\n","\n","        if self.bias:\n","            self.offset = nn.Parameter(torch.zeros(d))\n","            self.register_parameter(\"offset\", self.offset)\n","\n","    def forward(self, x):\n","        if self.p < 0. or self.p > 1.:\n","            norm_x = x.norm(2, dim=-1, keepdim=True)\n","            d_x = self.d\n","        else:\n","            partial_size = int(self.d * self.p)\n","            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n","\n","            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n","            d_x = partial_size\n","\n","        rms_x = norm_x * d_x ** (-1. / 2)\n","        x_normed = x / (rms_x + self.eps)\n","\n","        if self.bias:\n","            return self.scale * x_normed + self.offset\n","\n","        return self.scale * x_normed"],"metadata":{"id":"FFw8FZUQlFf_","executionInfo":{"status":"ok","timestamp":1727661084157,"user_tz":-330,"elapsed":727,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","batch_example = torch.randn(2, 5)"],"metadata":{"id":"dFaIvv0dletW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## feedforward network with 2 layer 3,144 first layer and 786 output layer\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"UsOQu-UolwdJ","executionInfo":{"status":"ok","timestamp":1727661089994,"user_tz":-330,"elapsed":729,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["MODEL_CONFIG = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 9,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"YWmXYwXGqvoo","executionInfo":{"status":"ok","timestamp":1727661095816,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["ffn = FeedForward(MODEL_CONFIG)\n","\n","# input shape: [batch_size, num_token, emb_size]\n","x = torch.rand(2, 3, 768)\n","out = ffn(x)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDjxuCsdqg-r","executionInfo":{"status":"ok","timestamp":1727645688107,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"e0218ba2-f1b8-454f-b950-96ad0a5312e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 768])\n"]}]},{"cell_type":"code","source":["## transformer block\n","## combine multihead attention layer normalization feedforward network\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n","        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x"],"metadata":{"id":"zPPvgy6dqqkD","executionInfo":{"status":"ok","timestamp":1727661102267,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n","block = TransformerBlock(MODEL_CONFIG)\n","output = block(x)\n","\n","print(\"Input shape:\", x.shape)\n","print(\"Output shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__F9V49S28Qs","executionInfo":{"status":"ok","timestamp":1727661127843,"user_tz":-330,"elapsed":717,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"841143d1-d35a-418e-8db1-be3fa0f3817f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([2, 4, 768])\n","Output shape: torch.Size([2, 4, 768])\n"]}]},{"cell_type":"code","source":["\n","class ODIA_Model(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"ctgfvW-d3Asr","executionInfo":{"status":"ok","timestamp":1727661132453,"user_tz":-330,"elapsed":722,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","batch = []\n","\n","txt1 = \"ଆଇସିସି ବିଶ୍ବକପରେ ସେ ଅଂଶ ଗ୍ରହଣ କରିଥିବାରୁ ସେ ନିଜକୁ ଭାଗ୍ୟଶାଳୀ ମନେ କରୁଛନ୍ତି \"\n","txt2 = \"କରୀନା କପୂର ହେଉଛନ୍ତି ଭାରତୀୟ କ୍ରିକେଟ ଟିମର ପୂର୍ବତନ ଅଧିନାୟକ ମସୁର ଅଲ୍ଲୀ କରୁଛ \"\n","\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7m-PGMFr4bj-","executionInfo":{"status":"ok","timestamp":1727661137472,"user_tz":-330,"elapsed":929,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"cf603d87-211c-4842-c831-8e72b80fbcea"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[156, 105, 228, 156, 105, 229, 156, 105, 116, 156, 105, 123, 156, 105,\n","         116, 156, 105, 123, 220, 156, 105, 105, 156, 105, 123, 156, 105, 114,\n","         156, 255, 235, 156, 105, 105, 156, 105, 243, 156, 105, 103, 156, 105,\n","         108, 156, 255, 229, 220, 156, 105, 116, 156, 255, 229, 220, 156, 105,\n","         227, 156, 105, 224, 156, 105, 114, 220, 156, 105, 245, 156, 255, 235,\n","         156, 105, 108, 156, 105, 117, 156, 105,  96, 220, 156, 105, 243, 156,\n","         105, 108, 156, 105, 123, 156, 105,  98, 156, 105, 123, 156, 105, 105,\n","         156, 105, 122, 156, 105, 108, 156, 255, 223, 220, 156, 105, 116, 156,\n","         255, 229, 220, 156, 105, 101, 156, 105, 123, 156, 105, 250, 156, 105,\n","         243, 156, 255, 223, 220, 156, 105, 255, 156, 105, 122, 156, 105, 245,\n","         156, 255, 235, 156, 255, 253, 156, 105, 114, 156, 105, 122, 156, 105,\n","         111, 156, 255, 222, 220, 156, 105, 106, 156, 105, 101, 156, 255, 229,\n","         220, 156, 105, 243, 156, 105, 108, 156, 255, 223, 156, 105, 249, 156,\n","         105, 101, 156, 255, 235, 156, 105,  97, 156, 105, 123, 220],\n","        [156, 105, 243, 156, 105, 108, 156, 255, 222, 156, 105, 101, 156, 105,\n","         122, 220, 156, 105, 243, 156, 105, 103, 156, 255, 224, 156, 105, 108,\n","         220, 156, 105, 117, 156, 255, 229, 156, 105, 231, 156, 105, 249, 156,\n","         105, 101, 156, 255, 235, 156, 105,  97, 156, 105, 123, 220, 156, 105,\n","         255, 156, 105, 122, 156, 105, 108, 156, 105,  97, 156, 255, 222, 156,\n","         255, 253, 220, 156, 105, 243, 156, 255, 235, 156, 105, 108, 156, 105,\n","         123, 156, 105, 243, 156, 255, 229, 156, 105, 253, 220, 156, 105, 253,\n","         156, 105, 123, 156, 105, 106, 156, 105, 108, 220, 156, 105, 103, 156,\n","         255, 224, 156, 105, 108, 156, 255, 235, 156, 105, 105, 156, 105,  97,\n","         156, 105, 101, 220, 156, 105, 227, 156, 105, 100, 156, 105, 123, 156,\n","         105, 101, 156, 105, 122, 156, 255, 253, 156, 105, 243, 220, 156, 105,\n","         106, 156, 105, 116, 156, 255, 223, 156, 105, 108, 220, 156, 105, 227,\n","         156, 105, 110, 156, 255, 235, 156, 105, 110, 156, 255, 222, 220, 156,\n","         105, 243, 156, 105, 108, 156, 255, 223, 156, 105, 249, 220]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = ODIA_Model(MODEL_CONFIG)\n","\n","out = model(batch)\n","print(\"Input batch:\\n\", batch)\n","print(\"\\nOutput shape:\", out.shape)\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_k6WRNwu3Y3W","executionInfo":{"status":"ok","timestamp":1727661147789,"user_tz":-330,"elapsed":4089,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"0db0e3fe-4036-451b-dc73-107e72b806b1"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Input batch:\n"," tensor([[156, 105, 228, 156, 105, 229, 156, 105, 116, 156, 105, 123, 156, 105,\n","         116, 156, 105, 123, 220, 156, 105, 105, 156, 105, 123, 156, 105, 114,\n","         156, 255, 235, 156, 105, 105, 156, 105, 243, 156, 105, 103, 156, 105,\n","         108, 156, 255, 229, 220, 156, 105, 116, 156, 255, 229, 220, 156, 105,\n","         227, 156, 105, 224, 156, 105, 114, 220, 156, 105, 245, 156, 255, 235,\n","         156, 105, 108, 156, 105, 117, 156, 105,  96, 220, 156, 105, 243, 156,\n","         105, 108, 156, 105, 123, 156, 105,  98, 156, 105, 123, 156, 105, 105,\n","         156, 105, 122, 156, 105, 108, 156, 255, 223, 220, 156, 105, 116, 156,\n","         255, 229, 220, 156, 105, 101, 156, 105, 123, 156, 105, 250, 156, 105,\n","         243, 156, 255, 223, 220, 156, 105, 255, 156, 105, 122, 156, 105, 245,\n","         156, 255, 235, 156, 255, 253, 156, 105, 114, 156, 105, 122, 156, 105,\n","         111, 156, 255, 222, 220, 156, 105, 106, 156, 105, 101, 156, 255, 229,\n","         220, 156, 105, 243, 156, 105, 108, 156, 255, 223, 156, 105, 249, 156,\n","         105, 101, 156, 255, 235, 156, 105,  97, 156, 105, 123, 220],\n","        [156, 105, 243, 156, 105, 108, 156, 255, 222, 156, 105, 101, 156, 105,\n","         122, 220, 156, 105, 243, 156, 105, 103, 156, 255, 224, 156, 105, 108,\n","         220, 156, 105, 117, 156, 255, 229, 156, 105, 231, 156, 105, 249, 156,\n","         105, 101, 156, 255, 235, 156, 105,  97, 156, 105, 123, 220, 156, 105,\n","         255, 156, 105, 122, 156, 105, 108, 156, 105,  97, 156, 255, 222, 156,\n","         255, 253, 220, 156, 105, 243, 156, 255, 235, 156, 105, 108, 156, 105,\n","         123, 156, 105, 243, 156, 255, 229, 156, 105, 253, 220, 156, 105, 253,\n","         156, 105, 123, 156, 105, 106, 156, 105, 108, 220, 156, 105, 103, 156,\n","         255, 224, 156, 105, 108, 156, 255, 235, 156, 105, 105, 156, 105,  97,\n","         156, 105, 101, 220, 156, 105, 227, 156, 105, 100, 156, 105, 123, 156,\n","         105, 101, 156, 105, 122, 156, 255, 253, 156, 105, 243, 220, 156, 105,\n","         106, 156, 105, 116, 156, 255, 223, 156, 105, 108, 220, 156, 105, 227,\n","         156, 105, 110, 156, 255, 235, 156, 105, 110, 156, 255, 222, 220, 156,\n","         105, 243, 156, 105, 108, 156, 255, 223, 156, 105, 249, 220]])\n","\n","Output shape: torch.Size([2, 194, 50257])\n","tensor([[[-0.7027,  1.4292,  0.0364,  ..., -0.2946, -0.4050, -0.4890],\n","         [-0.0856,  0.9867,  0.5689,  ...,  0.7176,  0.2533, -0.1809],\n","         [ 0.2307, -0.5992, -0.1972,  ...,  0.7753,  0.1377, -1.0965],\n","         ...,\n","         [-0.2519, -0.2954, -0.0179,  ...,  0.7979,  0.2576, -0.1476],\n","         [-0.3040,  1.2859,  1.8147,  ...,  0.3488,  0.0743,  0.3525],\n","         [-0.4908, -0.1500, -0.0107,  ...,  0.5765, -0.7083,  0.0068]],\n","\n","        [[-0.9916,  1.6954,  0.3281,  ..., -0.3329, -0.2919, -0.3090],\n","         [-0.0434,  0.8979,  0.2199,  ...,  0.9233,  0.0061,  0.3766],\n","         [-0.8419,  0.4366, -0.2061,  ...,  0.7075, -0.8394, -1.0410],\n","         ...,\n","         [-0.3940, -0.3271, -0.0390,  ...,  0.8348,  0.4070, -0.0292],\n","         [ 0.2597,  0.7133,  1.1914,  ..., -0.1323, -0.6971,  0.6332],\n","         [-0.6874, -0.0931, -0.3083,  ...,  0.0789, -0.9141, -0.1002]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["## total para metars\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiS2L8Sl4ENy","executionInfo":{"status":"ok","timestamp":1727598541918,"user_tz":-330,"elapsed":501,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"34048dad-01fa-45ca-ffb4-d98ac28054ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 141,738,240\n"]}]},{"cell_type":"code","source":["total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n","print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mhsbbwi_5bQ7","executionInfo":{"status":"ok","timestamp":1727598579848,"user_tz":-330,"elapsed":616,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"c002b12b-0e5d-4a4b-869b-124f434b1afb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable parameters considering weight tying: 103,140,864\n"]}]},{"cell_type":"code","source":["## now devide data train and test\n","## use 90% of data for traning purpose and 10% data for validation\n","\n","## read odia data\n","with open(\"/content/odia_data _ind.txt\",\"r\") as f:\n","    odia_language_data = f.read()\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(odia_language_data))\n","train_data = odia_language_data[:split_idx]\n","val_data = odia_language_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader(\n","    train_data,\n","    batch_size=10,\n","    max_length=MODEL_CONFIG[\"context_length\"],\n","    stride=MODEL_CONFIG[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader(\n","    val_data,\n","    batch_size=10,\n","    max_length=MODEL_CONFIG[\"context_length\"],\n","    stride=MODEL_CONFIG[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"AFoXiBhX5ksK","executionInfo":{"status":"ok","timestamp":1727661180234,"user_tz":-330,"elapsed":919,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAEH8BLE73aB","executionInfo":{"status":"ok","timestamp":1727645841481,"user_tz":-330,"elapsed":535,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"12f83772-3880-488b-98af-b07545461841"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","\n","Validation loader:\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n","torch.Size([4, 1024]) torch.Size([4, 1024])\n"]}]},{"cell_type":"code","source":["train_tokens = 0\n","for input_batch, target_batch in train_loader:\n","    train_tokens += input_batch.numel()\n","\n","val_tokens = 0\n","for input_batch, target_batch in val_loader:\n","    val_tokens += input_batch.numel()\n","\n","print(\"Training tokens:\", train_tokens)\n","print(\"Validation tokens:\", val_tokens)\n","print(\"All tokens:\", train_tokens + val_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZOzfuLr7tC2","executionInfo":{"status":"ok","timestamp":1727645850527,"user_tz":-330,"elapsed":612,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"f140e263-feb1-489e-cf98-def741c1a8ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training tokens: 520192\n","Validation tokens: 57344\n","All tokens: 577536\n"]}]},{"cell_type":"code","source":["## calculate losss\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches"],"metadata":{"id":"kAooUcCh7ydQ","executionInfo":{"status":"ok","timestamp":1727661196682,"user_tz":-330,"elapsed":686,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["## this function give every word to unique id\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'</EOS>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","\n","## this function decode id to word\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())"],"metadata":{"id":"rK1Gho3JvNNe","executionInfo":{"status":"ok","timestamp":1727661202855,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["## train the model\n","## using adam optimizer\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ABXSE7VMuOZF","executionInfo":{"status":"ok","timestamp":1727661206752,"user_tz":-330,"elapsed":1129,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss"],"metadata":{"id":"UtSQjS2bmlub","executionInfo":{"status":"ok","timestamp":1727661215950,"user_tz":-330,"elapsed":712,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"U0YNxXSHud2d","executionInfo":{"status":"ok","timestamp":1727661222503,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = ODIA_Model(MODEL_CONFIG)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","     tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"id":"k92WTlVgui48","executionInfo":{"status":"error","timestamp":1727661340084,"user_tz":-330,"elapsed":114808,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"446b0eec-fccc-4664-a2b3-1622b2fd2c1e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 5.060, Val loss 5.056\n","Ep 1 (Step 000005): Train loss 2.643, Val loss 2.659\n","Ep 1 (Step 000010): Train loss 1.926, Val loss 1.920\n","Ep 1 (Step 000015): Train loss 1.573, Val loss 1.565\n","Ep 1 (Step 000020): Train loss 1.472, Val loss 1.447\n","Ep 1 (Step 000025): Train loss 1.412, Val loss 1.402\n","Ep 1 (Step 000030): Train loss 1.398, Val loss 1.385\n","Ep 1 (Step 000035): Train loss 1.385, Val loss 1.369\n","Ep 1 (Step 000040): Train loss 1.373, Val loss 1.365\n","Ep 1 (Step 000045): Train loss 1.371, Val loss 1.358\n","Ep 1 (Step 000050): Train loss 1.384, Val loss 1.352\n","Ep 1 (Step 000055): Train loss 1.366, Val loss 1.347\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-030376619dc4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-ce739ea00a9a>\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, tokenizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Optional evaluation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 train_loss, val_loss = evaluate_model(\n\u001b[0m\u001b[1;32m     24\u001b[0m                     model, train_loader, val_loader, device, eval_iter)\n\u001b[1;32m     25\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-4a01ba99ef18>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, train_loader, val_loader, device, eval_iter)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-9955f250e9b2>\u001b[0m in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx"],"metadata":{"id":"eyAUOoOyv7cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(\"cpu\")\n","model.eval()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(\"କାକା ଆଚନ୍ତ୍ରାଳଭା ଗ୍ରଂଗୋରି\", tokenizer),\n","    max_new_tokens=60,\n","    context_size=MODEL_CONFIG[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id_Z2hZY6bR8","executionInfo":{"status":"ok","timestamp":1727649585358,"user_tz":-330,"elapsed":19583,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"09dceed2-d701-4c32-e09c-bcdc06f73e38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," କାକା ଆଚନ୍ତ୍ରାଳଭା ଗ୍ରଂଗୋରିକକରକରିକ୍ଯରର୍ଯାନ୍କାରା\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"model_weight.pth\")"],"metadata":{"id":"zp8ZbA9w7Tbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ODIA_Model(MODEL_CONFIG)\n","model.to(device)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.load_state_dict(torch.load(\"model_weight.pth\", map_location=device, weights_only=True))\n","model.eval();"],"metadata":{"id":"75dKVchR8qQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GfwdrdgQ_w3e"},"execution_count":null,"outputs":[]}]}